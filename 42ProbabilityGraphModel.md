# 1.概率模型

本文将讨论一些著名的概率模型。条件随机场是建立在这些方法的基本思想和概念之上的。

贝叶斯模型是一种依赖于几个特征值对单类变量进行分类的方法。在该模型中，假设输入值是有条件且独立的。这是一种所谓的生成方法，建模输入值$\vec{x}$和类变量$y$的联合概率$p(y、\vec{x})$。隐马尔可夫模型是对贝叶斯模型的扩展，也将变量$\vec{x}$和$\vec{y}$的依赖关系表示为联合概率分布。

由于计算复杂度，联合概率建模存在缺点。相比之下，最大熵模型是基于条件概率$p(y、x)$的建模。与贝叶斯模型一样，它是一种依赖于多个特征值对单个类变量进行分类的方法。差异在于考虑的是条件概率$p(y|x)$，而不是联合概率。

隐马尔可夫模型是对贝叶斯模型的顺序扩展，而条件随机场可以理解为对最大熵模型的顺序扩展。最大熵模型和条件随机场都被称为歧视性的方法。

## 

## 1.1贝叶斯模型

条件概率模型是具有输入向量$\vec{x}=(x_1,...,x_m)$的概率分布$p(y|\vec{x})$，其中$x_i(1\leq{i}\leq{m})$是特征变量，$y$是要预测的类变量。这个概率可以用贝叶斯定律来表示：

$$ p(y|\vec{x})=\frac{p(y)p(\vec{x}|y)}{p(\vec{x})}（1） $$ 分母p(x)对于分类并不重要，因为它可以理解为一个归一化常数，可以通过考虑$y$的所有可能值来计算。

该分子也可以写成一个联合概率

$$ p(y)p(\vec{x}|y) =p(y,\vec{x}) （2） $$ 这可能太复杂，无法直接计算(特别是当$x$中的组件数量很高时)。这个概率的一般分解可以用应用链规则表示出来

$p(x_1,...,x_m)=\displaystyle\prod_{i=2}^{m}{p(x_i-_1,...,x_1)}$:

$$ p(y,\vec{x})=p(y)\displaystyle\prod_{i=2}^{m}{p(x_i|x_i-_1,...,x_1,y)}（3） $$ 在实践中，通常假定，所有的输入变量$xi$有条件地相互独立，这被称为贝叶斯假设。这意味着$p（x_i|y, x_j)=p(x_i|y)$适用于所有的$i\neq{j}$。在此简化的基础上，建立了一个模型，它被称为贝叶斯分类器，它被表述为：

$$ p(y|\vec{x})\propto{p(y,\vec{x})=p(y)\displaystyle\prod_{i=1}^{m}{p(x_i|y)}}（4） $$ 这个概率分布没有方程3中表述的那么复杂，输入变量x之间的依赖关系没有被建模，这可能导致了对现实世界的不完美表示。然而，贝叶斯模型在许多现实世界的应用中表现惊人，如电子邮件分类。

## 

## 1.2隐马尔可夫模型

在贝叶斯模型中，只考虑了单个输出变量。预测一类变量的序列$y=(y_1,...,y_n)$的观察序列$x=(x_1,...,x_n)$。一个简单的序列模型可以表述为单个贝叶斯模型的乘积。不考虑单个序列位置之间的依赖关系。注意，与贝叶斯模型相比，每个序列位置只有一个特征，即各自观察结果的一致性：

$$ p(\vec{y},\vec{x})=\displaystyle\prod_{i=1}^{n}{p(y_i)*p(x_i|y_i)}（5） $$ 每个观察$x_i$只取决于类变量$y_i$在各自的序列位置上。由于这种独立性假设，从一个步骤到另一个步骤的转移概率不包括在这个模型中。事实上，这种假设在实践中几乎没有得到满足，这导致了这种模型的性能有限。因此，可以合理地假设在连续序列位置的观测结果之间存在依赖关系。为了建立这个模型，添加了状态转换概率：

$$ p(\vec{y},\vec{x})=\displaystyle\prod_{i=0}^{n}{p(y_i|y_i-_1)*p(x_i|y_i)}（6） $$ 这就导致了众所周知的隐马尔可夫模型:

$$ P(\vec{x})=\Sigma_{y\in\gamma}\displaystyle\prod_{i=0}^{n}{p(y_i|y_i-_1)*p(x_i|y_i)}（7） $$ 其中λ是所有可能的标签序列$\vec{y}$的集合。

独立输出变量$\vec{y}$被建模。其中一个缺点是，由于复杂性问题，输入变量$\vec{x}$之间存在条件独立（见方程(6))。我们稍后将看到，crf完全解决了这个问题。

## 

## 1.3最大熵模型

在第1.1节和第1.2节中引入的两个模型被训练以最大化联合。下面是最大熵模型更详细的讨论，因为它从根本上与crf相关。最大熵模型是一个条件概率模型，它基于最大熵原理，该原理指出，如果关于概率分布的不完整信息可用，唯一的无偏假设是在给定现有信息的情况下尽可能均匀的分布。在此假设下，适当的概率分布是给定训练材料约束的熵最大化的概率分布。对于条件模型$p(y|x)$，条件熵$H(y|x)$，其定义为：

$$ H(y|x)=-\sum_{(x,y)\in{Z}}p(y,x)logp(y|x)（8） $$ 集合Z由所有的输入变量x的集合和所有输出变量y的集合组成。

最大熵模型背后的基本思想是找到模型$p∗ (y|x)$它一方面具有最大可能的条件熵。目标函数，后来被称为原始问题，因此是这样的：

$$ p^*(y|x)=argmax_{p(y|x)\in{P}}H(y|x)（9） $$ 其中，P为与训练材料一致的所有模型的集合。

培训材料用各种特点来表示。在这里，这些被定义为二项值函数$f_i(x,y)\in{(0,1)}(1\leq{i}\leq{m})$，它同时依赖于输入变量x和类变量y。

每个特性的期望值$f_i$是由经验分布$p˜(x，y)$估计的。经验分布是通过简单地计算变量的不同值在训练数据中发生的频率来获得的：

$$ E\widetilde(f_i)=\sum_{(x,y)\in{Z}}p\widetilde(x,y)f_i(x,y)（10） $$ 这里考虑了所有可能的数据对$(x,y)$。由于训练材料中不包含的一对$(x,y)$的经验概率为0，$E˜(f_i)$可以重写为:

$$ E\widetilde(f_i)=\frac{1}{N}\sum_{(x,y)\in{T}}f_i(x,y)（11） $$ 训练集的大小为$N=|T|$。因此，$E\widetilde(f_i)$可以通过计算特征$f_i$的频率来计算，在训练集数据中发现值为1，然后用其除以训练集的大小N。

类似于方程(10)，模型分布上的一个特征的期望值被定义为:

$$ E(f_i)=\sum_{(x,y)\in{Z}}p(x,y)f_i(x,y)（12） $$ 与方程(10)（经验分布上的期望值）相比，这里考虑了模型分布。当然，$p(x,y)$一般不能计算，因为所有可能的x的数量是巨大的。这可以通过重写$E(f_i)$来解决:

$$ E(f_i)=\sum_{(x,y)\in{Z}}p(x)p(y|x)f_i(x,y)（13） $$ 并用经验分布p~(x)代替p(x)。这可以计算$E(f_i)$的近似值。这将导致：

$$ E(f_i)=\sum_{(x,y)\in{Z}}p\widetilde(x)p(y|x)f_i(x,y)（14） $$ 可以（类似于等式(12)）被转换为:

$$ E(f_i)=\frac{1}{N}\sum_{x\in{T}}\sum_{y\in{Y}}p(y|x)f_i(x,y)（15） $$ 只考虑在训练数据中出现的x的值(x∈T)，而所有可能的y值被纳入集合(y∈Y)。在许多应用程序中，集合Y通常只包含少量的变量。因此，这里对y和$E(f_i)$是可能被有效地计算出来。

方程(9)假设模型$p∗ (y|x)$与在培训材料中发现的证据一致。这意味着，对于每个特性$f_i$, 它对经验的预期价值分布必须等价于其对特定模型分布的期望值，这是第一个m约束：

$$ E(f_i)=E\widetilde(f_i)（16） $$ 另一个约束条件要由一个适当的条件概率来保证：

$$ p(y|x)\geq0~~for~~all~~x,y~~~~~~~~\sum_{y\in{Y}}p(y|x)=1~~for~~all~~x（17） $$ 查找这些约束下的$p*(y|x)$可以表述为一个约束优化问题。对于每个约束条件，都有一个拉格朗日乘子$\lambda_i$引入。这就导致了以下拉格朗日函数$(p,\vec{\lambda})$:

$$ \bigwedge(p,\vec{\lambda})=H{(y|x)}+\sum _{i=1}^{m}\lambda_i(E(f_i)-E\widetilde(f_i))+\lambda_m+*1(\sum*{y\in{\gamma}}p(y|x)-1)（18） $$ （推导过程略）

所以，最大熵模型可以表示为： $$ p_\vec{\lambda}*(y|x)=\frac{1}{Z_\vec{\lambda}(x)}exp(\sum_{i=1}\lambda_if_i(x,y))（19） $$ $Z_\vec{\lambda}(X)$是：

$$ Z_\vec{\lambda}(X)=\sum_{y\in{Y}}exp(\sum_{i=1}\lambda_if_i(x,y)（20） $$ 第2节从另一个角度讨论了条件概率分布公式和指数加权特征的乘积。在第3节中，条件

随机场也是对数线性模型，其与最大熵模型的相似性变得更加明显。

本节介绍了两种概率模型。一方面是基于联合概率分布的生成模型，如贝叶斯模型和隐马尔可夫模型。从公式(4)和公式(6)中可以看出，在这些模型中，观测变量$x_i$也称为“生成”，即输入变量y。另一方面，判别模型，如最大熵模型，都是基于条件概率分布的。在下一节中，我们将从不同的角度回顾这两组模型它们的图形表示。

# 

# 2.图形表示

概率模型的基本概率分布可以用图形形式表示，这就是为什么它们通常被称为概率图模型。

一个概率图形模型是一个概率分布的图解表示。在这样的图中，每个随机变量都有一个节点。两个变量之间的边代表了这些变量之间的条件独立性。条件独立性意味着如果两个随机变量a和b在条件上是独立的，则给定第三个随机变量c是独立的概率分布，即$p(a,b|c)=p(a|b)p(b|c)$。这样的图表，也被称为独立图，我们可以读取底层分布的条件独立属性。注意，一个全连通的独立图不包含任何关于概率分布的信息，只有边的缺失才会提供信息：概率分布中的条件独立性不会导致图中边的缺失。

条件独立性是一个重要的概念，因为它可以用来将复杂的概率分布分解为因子的乘积，每个因子由相应的随机变量的子集组成。这个概念使得复杂的计算变得更加有效。一般来说，分解，实际上是概率分布的一个因式分解，是写成因子$\Psi_s$，与$\vec{v_s}$构成该因子的各自随机变量的子集：

$$ p(\vec{v})=\displaystyle\prod_{s}\Psi_s(\vec{v_s})（21） $$ 设$G=(V，E)$是一个有顶点V和边的E的图。在一个独立图中，顶点$V=X\cup{Y}$，X和Y是随机变量集，用圆表示。X通常被认为是输入或观察变量集（阴影圆圈），Y是输出变量集（空节点）。独立图可以有有向边或无向边，这取决于它所代表的图形模型的类型。

在一个因子图中，圆表示独立图中用圆表示的底层分布的随机变量。此外，一个因子图包含因子节点，用小的填充方块表示，它表示因子$\Psi_s$。在因子图中，边总是无向的，将随机变量链接到因子节点。一个因子节点$\Psi_s$包括各自的因子节点通过一条边直接连接到的所有随机变量。因此，一个因子图更明确地表示了底层概率分布的因子分解。有向和无向图形模型的不依赖性图都可以转换为因子图。

例如，假设一个概率分布$p(x_1,x_2,y)$分解为$p(\vec{x})=p(x_1)p(x_2)p(y|x_1,x_2)$。它有因子$\Psi_1(x_1)=p(x_1)$，$\Psi_2(x_2)=p(x_2)$，和$\Psi_3(y)=p(y|x_1,x_2)$。在这里，$x_1$和$x_2$由y给定且条件独立。下图图1显示了一个独立图和一个代表这个分布的因子图。

![Alt text]figure/图1.png

​                                                                                   图1

下面将讨论有向和无向图形模型。贝叶斯和隐马尔可夫模型属于第一组，最大熵模型属于第二组图形模型。

## 

## 2.1定向图模型

一个联合分布$p(\vec{v})$可以分解为每个节点$v_k$条件分布的乘积，因此每个这样的条件分布都以其父节点集$v_k^p$为条件 :

$$ p(\vec{v})=\displaystyle\prod_{k=1}^{K}p(v_k|v_k^p)（22） $$ 这是如图1所示的示例分布$p(x_1,x_2,y)$。作为另一个例子，以第1.1节中讨论的贝叶斯分类器为例，下图图2表示了这样一个有三个观察变量的模型。相应的概率分布分解为$p(y,x_1,x_2,x_3)=p(y)*p(x_1|y)\*p(x_2|y)\*p(x_3|y)$，遵循贝叶斯假设。类似的，图3显示了三个变量$x_1$，$x_2$，$x_3$序列的HMM分类器。因子分解为$p(x_1,x_2,x_3,y_1,y_2,y_3)=\Psi_1(y_1)\*\Psi_2(x_1,y_1)\*\Psi_3(x_2,y_2)*\Psi_4(x_3,y_3)*\Psi_5(y_1,y_2)*\Psi_6(y_2,y_3)$，分别对应到HMM（见公式(6)）。

![Alt text]figure/图2.png

​                                                                                             图2

![Alt text]figure/图3.png

​                                                                                             图3

## 

## 2.2无向图模型

概率分布可以用无向图形模型表示，使用非负函数的乘积。分解的方式是条件独立节点不出现在同一因子内，这意味着它们属于不同的小系：

$$ p(\vec{v})=\frac{1}{Z}\displaystyle\prod_{C\in{c}}\Psi_C(\vec{v}_c)（23） $$ 因子$\Psi_C\geq0$被称为势函数，属于小集团$C\in{c}$中的随机变量$\vec{v}_C$。

势函数可以是任何函数。由于这种一般性，势函数不一定是概率函数。这与向图相反，其中联合分布将其分解为条件分布的乘积。因此，势函数乘积的归一化在实现一个适当的概率测度中是必要的。计算Z是参数学习过程中的主要挑战之一，所以对所有可能的变量求和是必要的：

$$ Z=\sum_{\vec{v}}\displaystyle\prod_{C\in{c}}\Psi_C(\vec{v}_C)（24） $$ 在第1.3节中讨论了最大熵模型，它可以用非负势函数的乘积来表示：

$$ p_{\vec{\lambda}}(y|x)=\frac{1}{Z_{\vec{\lambda}}(x)}\displaystyle\prod_{i=1}^{m}exp(\lambda_if_i(x,y))（25） $$ 在这种对数线性模型中，势函数被表示为加权特征的指数函数。这种公式经常被使用,因为它满足势函数严格正的要求。下图图4(a)显示了具有观测变量x的最大熵分类器的独立图，和具有三个特征的相应因子图如图4(b)所示。

![Alt text]figure/图4.png
​                                                                                          图4

有向和无向图形模型的原始概率分布的分解方式不同。在有向图形模型中所做的那样，分解成条件概率分布的乘积是直接的。在无向图形模型中，将其分解为任意函数。这不需要明确说明变量如何关联。但它是以必须计算标准化因子为代价的。

# 

# 3.条件随机字段

在前一节中，我们从数学的角度讨论了一些著名的概率模型。此外，还显示了描述模型的潜在概率分布的图形表示。

隐马尔可夫模型可以理解为贝叶斯模型的序列版本：隐马尔可夫模型建模为线性决策序列，而不是单一的独立决策。因此，条件随机场可以理解为最大熵模型的序列版本，即它们也是判别模型。此外，与隐马尔可夫模型相比，条件随机场并不与线性序列结构相连，而是可以任意结构化。

接下来阐述了条件随机场的思想和理论基础。首先，给出了条件随机场的一般公式，然后深入讨论了最流行的crf形式，即那些具有线性序列结构的crf形式。一个主要的焦点是训练和推理的方面。本节以简要讨论任意结构化的crf结束。

## 

## 3.1基本原则

条件随机场(CRF)是计算概率$p(\vec{y}|\vec{x})$的概率模型，给定输入$\vec{x}=(x_1,...,x_n)$（也成为观察变量），计算可能输出$\vec{y}=(y_1,...y_n)$。CRF的一般情况可以由公式(23)推出：

$$ p(\vec{v})=\frac{1}{Z}\displaystyle\prod_{C\in{c}}\Psi_C(\vec{v}_c)（26） $$ 条件概率$p(\vec{y}|\vec{x})$可以写成：

$$ p(\vec{y}|\vec{x})=\frac{\frac{1}{Z}\displaystyle\prod_{C\in{c}}\Psi_C(\vec{x}*C,\vec{y}\*C)}{\frac{1}{Z}\sum\*{\vec{y}'}\displaystyle\prod*{C\in{c}}\Psi_C(\vec{x}_C,\vec{y}_C)}（27） $$ 由此，推导出了crf的一般模型公式：

$$ p(\vec{y}|\vec{x})=\frac{1}{Z(\vec{x})}\displaystyle\prod_{C\in{c}}\Psi_C(\vec{x}_C,\vec{y}_C)（28） $$ 如第2节中所述，$\Psi_C$是独立图中最大集团中对应的不同因素。见图5中线性链CRF的示例。每个因子对应于一个结合不同特征的势函数$f_i$考虑的观察和输出。归一化的结果来自于公式(27)的分母：

$$ Z(\vec{x})=\sum_{\vec{y}'}\displaystyle\prod_{C\in{c}}\Psi_C(\vec{x}_C,\vec{y}_C)（29） $$ 事实上，在训练和推理过程中，对于每个实例都使用了一个单独的图，它是由所谓的群系模板构建的。小集团模板通过定义小集团的组成来对基础数据的结构做出假设。每个群体都是一组假定的相互依赖的变量，即那些包含在相应的势函数中的变量。在第3.2节和第3.3节中给出了集团模板的示例。

## 

## 3.2线性链crf

CRF的一种特殊形式，其结构为线性链，将输出变量建模为一个序列。图5显示了各自的独立性和因子图。公式(28)中引入的CRF可以表示为：

$$ p(\vec{y}|\vec{x})=\frac{1}{Z(\vec{x})}\displaystyle\prod_{j=1}^{n}\Psi_j(\vec{x},\vec{y})（30） $$ 与：

$$ Z(\vec{x})=\sum_{\vec{y}'}\displaystyle\prod_{j=1}^{n}\Psi_j(\vec{x},\vec{y}')（31） $$ 考虑到因子$\Psi_j(\vec{x},\vec{y})$的形式：

$$ \Psi_j(\vec{x},\vec{y})=exp(\sum_{i=1}^m\lambda_if_i(y_{i-1},y_i,\vec{x},j))（32） $$ 
![Alt text]figure/图5.png

​                                                                                            图5

并假设n+1为观测序列的长度，那么一个线性链CRF可以写成：

$$p_{\vec{\lambda}}(\vec{y}|\vec{x})=\frac{1}{Z_{\vec{\lambda}}(\vec{x})}exp(\sum_{j=1}^n\sum_{i=1}^m\lambda_if_i(y_{i-1},y_i,\vec{x},j))$$

与最大熵模型相比，需要索引j，因为需要考虑的是一个标签序列，而不是单个标签。在公式(33)中，j指定了输入序列$\vec{x}$的位置。请注意，权重为$\lambda_i$。这种技术称为参数绑定，用于确保一组指定的变量具有相同的值。

对[0,1]的归一化是由：

$$ Z_{\vec{\lambda}}(\vec{x})=\sum_{\vec{y}\in{Y}}exp(\sum_{j=1}^n\sum_{i=1}^m\lambda_if_i(y_{i-1},y_i,\vec{x},j))（34） $$ 详细信息为Y，选取所有可能的标签序列集，以得到一个可行的概率。

在公式(33)中，给出了一个线性链CRF的公式。将和移动到指数函数前面的序列位置上,CRF的实际分解变得更加明显：

$$ p_{\vec{\lambda}}(\vec{y}|\vec{x})=\frac{1}{Z_{\vec{\lambda}}(\vec{x})}\displaystyle\prod_{j=1}^{n}{exp(\sum_{j=1}^n\sum_{i=1}^m\lambda_if_i(y_{i-1},y_i,\vec{x},j))}（35） $$ 图5(b)中的因子图对应于这个因式分解。我们也可以在指数函数前面的不同特征上移动这个和：

$$ p_{\vec{\lambda}}(\vec{y}|\vec{x})=\frac{1}{Z_{\vec{\lambda}}(\vec{x})}\displaystyle\prod_{i=1}^{m}{exp(\sum_{j=1}^m\sum_{i=1}^n\lambda_if_i(y_{i-1},y_i,\vec{x},j))}（36） $$ 通过将这两个和移动到指数函数的前面，该模型可以用更多的因素来解释：

$$ p_{\vec{\lambda}}(\vec{y}|\vec{x})=\frac{1}{Z_{\vec{\lambda}}(\vec{x})}*\displaystyle\prod_{i=1}^{m}\displaystyle\prod_{j=1}^{n}{exp(\lambda_if_i(y_{i-1},y_i,\vec{x},j))}（37） $$ 由于图中有大量的因子，所以这里没有显示相应的因子图。

基于最大团的分解（见公式(35)）是通常应用于线性链CRF的方法。另外两个因素分解（见公式(36)和(37)）不符合这种最大值。一般来说，根据比最小团更少的变量节点组成的小团进行分解会导致不准确，因为不是所有的依赖关系都得到正确的考虑。因此，在这种情况下，它会导致冗余的计算，如公式(37)所示。本文的其余部分是基于第一个因素分解的思想。

## 

## 3.3任意结构化的crf

下面，将解释具有任意图形结构的crf，如树或网格结构。从线性链CRF移动到一般CRF基本上意味着消除小集团模板（见第3.1节）对线性结构建模的限制。因此，必须应用更一般的训练和推理算法。

### 

### 3.3.1展开图

在一些出版物中，描述了不同的CRF结构。必须强调的是，这些结构是典型的，因为实际图分别针对每个实例实例化，以便在团体模板的帮助下进行训练或推断。因此，实际的图结构取决于所考虑的实例和CRF的特定类型。潜在函数$\Psi_j$在模型中（与公式(30)进行比较）与小集团模板相关联，但与图中的小集团无关。为特定实例构建图的过程称为“展开图”。

正如在第3.2节中已经讨论的，线性链CRF的群模板集来自：

$$ c={(C)}~~with~~C={(\Psi_j(y_j,y_{j-1},\vec{x})|\forall{j\in(1,...,n)})}（38） $$ 因此，对于线性链CRF，只有一个团模板导致每个可能的输入值的线性结构。只有由于

这种线性序列结构，SFSA才可以作为有效实现的基础。

另一组可能的集团模板是：

$$ c={(C_1,C_2)~~with~~C_1=\Psi_j(y_j,y_{j-1},\vec{x})|\forall{j\in{(1,...,n)}}}~~and~~C_2={\Psi_{ab}(y_a,y_b,\vec{x})|(a,b)\in{(1,...,n)^2}}（39） $$ 其中，a和b是指定在输入序列上具有特殊属性的标签的索引。例如，在一个序列中，$\vec{x}=(x_1,...,x_n)\in{N^n}$，索引a和b可以指定其中b可被a整除的所有项。给定一个具体序列{2，3，4，5，6}，这将得到一个结构如图6所示的CRF。在实际应用程序中，经常应用参数绑定，在本例中，权重$\lambda_j$的值对于相同的小群模板也是相同的，独立于序列中的位置。

![Alt text]figure/图6.png

​                                                                                             图6

### 

### 3.3.2培训和推理

在序列结构中，如HMM或线性链CRF(这是一个以输入值$\vec{x}$为全局条件的简单链)，可以应用正向和维特比算法，它们基于沿链在只有两个可能的方向发送消息。除了概念上不同的算法，如采样方法，还有这些算法对树状结构图的推广，即和积和最大和算法。其基本思想是，沿着图发送的消息在转发之前要从不同的方向收集。这种推广也可以用于任意图。其基本思想是从原始图中计算出一个所谓的结树。然后，这些算法可以以一种稍微修改一下的形式进行应用。
