---
title: "推荐系统评估"
author: "罗涵艺"
date: "2021/10/22"
output: word_document
---

# 一、简介

随着互联网的迅速发展，网络信息量大幅增长，虽然满足在了用户在信息时代对于信息的需求，却也使得用户在面对大量信息时无法从中获得对自己真正有用的那部分信息，对信息的使用效率有所降低，从而产生信息超载等问题。因此推荐系统应运而生，它是根据用户的信息需求、兴趣等，将用户感兴趣的信息、产品等推荐给用户的个性化信息推荐系统，通过研究用户利用信息的偏好和角度，进行个性化计算，系统发现用户的兴趣点，从而引导用户发现自己的信息需求。

推荐系统评估是检验推荐系统性能的手段，对于推荐系统的发展具有重要的引导意义。推荐系统评估从推荐系统的精准度、稳定性等多个维度出发，来评估出推荐系统的实际效果及表现, 从中发现可能的优化点。进而通过优化推荐系统，期望更好地满足用户的诉求，为用户提供更优质的推荐服务，同时通过推荐获取更多的商业利益。


# 二、常用评估指标
推荐系统评估的指标主要从三个维度出发，用户的维度、平台方的维度以及推荐系统自身的维度。

## 用户的维度
一般来说，从用户维度有如下几类指标可以衡量推荐系统对用户的价值：

#### 准确度
准确度评估的是推荐的“标的物“是不是用户喜欢的。这里所说的准确度更多的是用户使用的主观体验感觉。

#### 惊喜度
所谓惊喜度，就是让用户有耳目一新的感觉，无意中给用户带来惊喜。

#### 新颖性
新颖性就是推荐用户之前没有了解过的“标的物”。人都是“喜新厌旧”的，推荐用户没接触过的东西，可以提升用户的好奇心和探索欲。

#### 信任度
信任度在这里对于推荐系统来说也是类似的，如果推荐系统能够满足用户的需求，用户就会信任推荐系统，会持续使用推荐系统来获取自己喜欢的“标的物”。

#### 多样性
用户的兴趣往往是多样的，在做推荐时需要给用户提供多“品类”的“标的物”，以挖掘用户新的兴趣点，拓展用户的兴趣范围，提升用户体验。

#### 体验流畅度
推荐系统是一个软件产品，用户的体验是否好，是否卡顿，响应是否及时，对用户的行为决策非常关键。
流畅的用户体验，是推荐服务的基本要求。但只要服务不稳定，响应慢，会极大影响用户体验，甚至导致用户卸载产品。
上面这些指标，有些是可以量化的(比如精准度、流畅度)，有些是较难量化的(比如惊喜度、新颖性)，所有这些指标汇聚成用户对推荐模块的满意度。

## 平台方的维度
对于平台方来说，商业目标是最重要的指标之一，平台方的盈利目的又需要借助用户来实现(不管是用户购买，还是广告，都需要有大量用户)，所以平台方除了关注绝对的收益外，还需要关注用户活跃、留存、转化、使用时长等用户使用维度的指标。推荐系统怎么更好的促进收益增长，促进用户活跃、留存、转化等就是平台方最关注的商业指标。
所以站在平台方角度看，最重要的指标主要有如下3类：

#### 用户行为相关指标；

#### 商业变现相关指标；

#### 商家(即“标的物”提供方)相关指标；



## 推荐系统自身维度
推荐系统是一套算法体系的闭环，通过该闭环为用户提供服务，从推荐系统自身来说，主要衡量指标包括如下：

#### 准确度
作为推荐系统核心的推荐算法, 本身是一种机器学习方法，不管是预测、分类、回归等机器学习问题都有自己的评估指标体系。

#### 实时性
用户的兴趣是随着时间变化的，推荐系统怎么能够更好的反应用户兴趣变化，做到近实时推荐用户需要的“标的物”是特别重要的问题。特别像新闻资讯、短视频等满足用户碎片化时间需求的产品，做到近实时推荐更加重要。

#### 鲁棒性
推荐系统一般依赖用户行为日志来构建算法模型，而用户行为日志中会包含很多开发过程中、系统、人为(比如黑客攻击)等产生的垃圾数据，推荐算法要具备鲁棒性，尽量少受“脏”的训练数据的影响，才能够为用户提供稳定一致的服务。

#### 响应及时稳定性
用户通过触达推荐模块，触发推荐系统为用户提供推荐服务，推荐服务的响应时长，推荐服务是否稳定(服务正常可访问，不挂掉)也是非常关键的。

#### 抗高并发能力
推荐系统是否能够承受高并发访问，在高并发用户访问下(比如双十一的淘宝推荐)，是否可以正常稳定的提供服务，也是推荐系统的重要能力。除了上面说的这些指标外，推荐模型的可维护性、可拓展性、模型是否可并行训练、需要的计算存储资源、业务落地开发效率等也是推荐业务设计中需要考虑的重要指标。

#### 覆盖率
从“标的物”提供方的角度来看，希望自己提供的“标的物”都能够被用户“相中”，不然这个“标的物”就没有任何价值。所以推荐系统需要将更多的“标的物”推荐(曝光)出去， 只有曝光出去，才有被用户“消费”的可能。

#### 挖掘长尾的能力
推荐系统的一个重要价值就是发现长尾(《长尾理论》.ChrisAnderson)，将小众的“标的物”分发给喜欢该类“标的物”的用户。度量出推荐系统挖掘长尾的能力，对促进长尾“标的物”的“变现”及更好地满足用户的小众需求从而提升用户的惊喜度非常有价值。

# 三、评估方法
推荐算法本质上就是一个机器学习问题，因此需要构建推荐算法模型，选择认为合适的算法模型，将算法模型部署到线上推荐业务中，利用算法模型来预测用户对“标的物”的偏好，通过用户的真实反馈来评估算法效果。主要的评估方法有离线评估和在线评估。

## 离线评估

### 离线评估的主要方法

#### Holdout检验
Holdout检验是基础的离线评估方法，它将原始的样本集合随机划分为训练集和验证集两部分，比如70%训练集，30%测试集（但现在很多机器学习框架、深度学习框架中都增加了验证集，即将整个数据集分成三份，70%训练集，10%验证集，20%测试集）。Holdout检验的缺点也很明显，即在验证集上计算出来的评估指标与训练集和测试机的划分有直接关系，如果仅进行少量Holdout检验，则得到的结论存在很大的随机性（在划分数据集的时候尽量保证其随机性）。

#### 交叉检验

##### k-fold交叉验证
先将全部样本划分成k个大小相等的样本子集，依次遍历这k个子集，每次都把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估，最后将所有k次的评估指标的平均值作为最终的评估指标，而在实际经验中，k经常取值为10。

##### 留一验证
每次留下1个样本作为验证集，其余所有样本作为测试集，样本总数为n，依次遍历所有n个样本，进行n次验证，再将评估指标求平均得到最终指标。在样本总数较多的情况下，留一验证法的时间开销极大，事实上，留一验证是留p验证的特例，留p验证是指每次留下p个样本作为验证集，而从n个元素中选择p个元素有$$C_n^p$$种可能，因此它的时间开销远远高于留一验证，故很少在实际中使用。

##### 自助法(Bootstrap)
不管是holdout检验还是交叉检验，都是基于划分训练集和测试集的方法进行模型评估的，当样本规模比较小时，将样本集进行划分，会进一步缩小训练集，有影响模型的训练效果。
自助法（Bootstrap）是基于自助采样法的检验方法：对于总数为n的样本集合，进行n次有放回的随机抽样，得到大小为n的训练集，在n次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集进行模型验证，就是自助法的验证过程。

### 离线评估的主要指标
离线评估是在推荐算法模型开发与选型的过程中对推荐算法做评估, 通过评估具体指标来选择合适的推荐算法, 将算法部署上线为用户提供推荐服务。具体可以评估的指标有：

#### 准确度指标(RMSE,MSE)
准确度评估的主要目的是事先评估出推荐算法模型的好坏(是否精准), 为选择合适的模型上线服务提供决策依据。
推荐系统准确度的评估也可以自然而然的采用推荐算法所属的不同机器学习范式来度量。而预测准确度可以用评分预测来表示
评分预测的预测准确度一般通过均方根误差（RMSE）和平均绝对误差（MAE）计算。对于测试集中一个用户u和物品i，令$Rui$是用户u对i的实际评分，而$Rui$ 是推荐算法给出的预测评分。那么 RMSE 的定义为：
$$ RMSE=\frac{\sum_{u,i∈T} （Rui-Rvi)^2 }{\sqrt{|T|}}$$
MAE 采用绝对值计算预测误差，它的定义为：
$$ MSE=\frac{\sum_{u,i∈T} |Rui-Rvi| }{{|T|}}$$
假设用列表 records 存放用户评分数据，令 records[i] = [u,i,$Rui$,$Pui$]，其中$Rui$是用户u对i的真实评分$Pui$是算法预测出来的评分，那么下面的代码分别展示了RMSE和MAE的计算过程:

1.	def RMSE(records):
2.	    return math.sqrt(
3.	        sum([(Rui - Pui)*(Rui - Pui) for u,i,Rui,Pui in records]) \
4.	            / float(len(records)))


1.	def MAE(records):
2.	    return sum([abs(Rui-Pui) for u,i,Rui,Pui in records]) \
3.	        / float(len(records))


#### 精准率和召回率(Precision&Recall)
精确率（Precision）是分类正确的正样本个数占分类器判定为正样本的样本个数的比例
召回率（Recall）是分类正确的正样本个数占真正正样本个数的比例

排序模型中，通常没有一个确定的阈值把预测结果直接判定为正样本还是负样本，而是采用 Top N 排序结果的精确率（Precision@N）和召回率（Recall@N）来衡量排序模型的性能，即认为模型排序的Top N的结果就是模型排定的正样本，然后计算精确率和召回率。
精确率和召回率是矛盾统一的两个指标：为了提高精确率，分类器需要尽量再“更有把握时”才把样本预测为正样本，但往往因为过于保守而漏掉很多“没有把握”的正样本，导致召回率降低。因此使用F1-score进行调和（也叫F-measure）。
推荐结果的召回率：
$$ Recall=\frac{Nrs}{Nr}$$
$$ Precision=\frac{Nrs}{N_s}$$
$$ F-measure=\frac{2*Precision*Recall}{Precision+Recall}$$
其中，Nrs是推荐物品中用户喜欢的个数、$N_s$是推荐的物品数，ｎ代表前ｎ个推荐项。$N_r$是用户喜欢的物品数。准确率描述的是用户喜欢的物品的比例，召回率描述的是不遗漏用户喜欢物品的比率，F1-score是准确率与召回率的一种折中。


#### 覆盖率指标(Coverage)
对于任何推荐范式，覆盖率指标都可以直接计算出来。覆盖率(Coverage)的具体计算公式如下：
$$ Coverage=\frac{∪u∈UR_u}{|I|}$$
其中U是所有提供推荐服务的用户的集合，I是所有“标的物”的集合，$R_u$是给用户u的推荐“标的物”构成的集合。
另定义物品的流行度为 $p(i)$，$I $为全部的物品集，假设物品是按照流行度$p$从小到大排列的。
则定义基尼系数(Gini Index)如下：
$$G=\frac{1}{n-1}\sum_{j=0}^n (2j-n-1)p(j) $$
相关代码实现：

1.	def GiniIndex(p):
2.	    j = 1
3.	    n = len(p)
4.	    G = 0
5.	    for item,weight in sorted(p.items(),key = itemgetter(1)):
6.	        G += (2 * j - n - 1) * weight
7.	    return G / float(n - 1)


#### 实时性指标
用户的兴趣是随着时间变化的，推荐怎么能尽快反应用户兴趣变化，捕捉用户新的兴趣点在日益竞争激烈的互联网时代对产品非常关键，特别是新闻、短视频这类APP，需要快速响应用户的兴趣变化。
一般来说，推荐系统的实时性分为如下四个级别T+1(每天更新用户推荐结果)、小时级、分钟级、秒级。越是响应时间短的对整个推荐系统的设计、开发、工程实现、维护、监控等要求越高。

1.	对于“侵占”用户碎片化时间的产品，这些产品用户“消耗”“标的物”的时间很短，因而建议推荐算法做到分钟级响应用户兴趣变化
2.	对于电影推荐、书推荐等用户需要消耗较长时间“消费”标的物的产品，可以采用小时级或者T+1策略
3.	一般推荐系统不需要做到秒级，但是在广告算法中做到秒级是需要的。

#### 鲁棒性指标（Robustness）
推荐系统是否受脏数据影响，是否能够稳定的提供优质推荐服务非常关键。为了提升推荐系统的鲁棒性，这里有四个提议：

1.	尽量采用鲁棒性好的算法模型；
2.	做好特征工程，事先通过算法或者规则等策略剔除掉可能的脏数据；
3.	在日志收集阶段，对日志进行加密，校验，避免人为攻击等垃圾数据引入；
4.	在日志格式定义及日志打点阶段，要有完整的测试case，做好冒烟回归测试，避免开发失误或者bug引入垃圾数据。

#### 对数损失函数(LogLoss)
LogLoss，在一个二分类问题中，LogLoss定义为：
其中$Yi$为输入实例$Xi$的真实类别，$Pi$为预测输入实例$Xi$是正样本的概率， N为样本总数。
LogLoss 是逻辑回归的损失函数，大量深度学习模型的输出层是逻辑回归或softmax，因此采用LogLoss作为评估指标能够非常直观的反映模型损失函数的变化，站在模型的角度来讲，LogLoss非常适于观察模型的收敛情况。

#### ROC曲线
ROC是“接收者操作特性”的缩写，最早是一种用于信号检测的分析工具。近年来ROC分析被广泛应用于信息检索与机器学习相关领域算法的评估。ROC曲线是个二维坐标图形，X轴是错误的正例率，Y轴是正确的正例率，ROC曲线直观的展示了FPR与TPR之间的对应关系。

#### 平均倒数排名（MRR)
平均倒数排名（Mean Reciprocal Rank,MRR）的概念来自于信息检索系统，即希望越相关的检索结果应该排在越前面。平均倒数排名应用于推荐系统时，可以度量推荐系统是否将用户最喜欢的物品 排在最前面，其定义如下：
$$ MRR=\frac{\sum_{q=1}^Q 1/ranki}{Q}$$

（Q为物品推荐的次数，ranki为用户最喜欢的物品在推荐列表中的排名，MRR的值越大，推荐系统的性能越好）

#### 斯皮尔曼相关系数(SRCC)
斯皮尔曼等级相关系数(Spearman Rank Correlation Coefficient,SRCC)用于计算推荐系统推荐物品的排民顺序与真实的排名顺序之间的皮尔逊相关系数。定义如下：
$$ SRCC=\frac{\sum_{i}(r1(i)-u1)(r2(i)-u2)}{\sqrt{\sum_{i}(r1(i)-u1)^2 }\sqrt{\sum_{i}(r2(i)-u2)^2 }}$$

#### 归一化折损累积利益
斯皮尔曼等级相关系数其实并没有考虑排名的位置，换句话说对所有的错误排名的惩罚是相同的。实际上，第１个位置与第３个位置的排名颠倒是不可接受的，但是第21个位置与第23个位置的颠倒可能就没那么严重了。因此，近几年归一化折损累积增益在推荐系统的评估越来越多地被采用，其定义如下：
$$ nDCG=\frac{DCG(r)}{DCG(r perpect)}$$
其中
$$DCGR(r)=\sum_{}dsic(r(i))u(i)  $$
$Disc（r(i))$是基于排名位置的折损位置，$u(i)$是推荐列表中每个位置物品的使用，$DCG（Rperfect)$代表了一个完美排名的折损累积增益。


虽然离线评估推荐系统有很多指标，但进行离线模型的评估时并不能使用全部的指标，没必要陷入「完美主义」和「实验室思维」的误区，选择2-4个适合自己业务的指标进行优化即可。

## 在线评估
推荐系统的评估的在线评估可以分为两个阶段：第一阶段和第二阶段

### 在线评估第一阶段
第一阶段是推荐算法上线服务到用户使用推荐产品这个阶段, 在这个阶段用户通过使用推荐产品触发推荐服务(平台通过推荐接口为用户提供服务)。这个阶段可以评估的指标主要有相应及时稳定性指标和抗高并发能力指标。

#### 响应及时稳定性指标
该指标是指推荐接口可以在用户请求推荐服务时及时提供数据反馈。服务器响应会受到很多因素影响，比如网络、CDN、Web服务器、操作系统、数据库、硬件等，一般无法保证用户的每次请求都控制在一定时间内。所以一般采用百分之多少的请求控制在什么时间内这样的指标来评估接口的响应时间(比如99%的请求控制在50ms之内)。
可以在web服务器(如Nginx)端对用户访问行为打点，记录用户每次请求的时长(需要在web服务器记录/配置接口请求响应时长)，将web服务器的日志上传到大数据平台，通过数据分析可以统计出每个接口的响应时长情况。

#### 抗高并发能力指标
当用户规模很大时，或者在特定时间点有大量用户访问时，在同一时间点有大量用户调用推荐服务，推荐接口的压力会很大，推荐系统能否抗住高并发的压力是一个很大的挑战。因此可以在接口上线前对接口做打压测试，事先了解接口的抗并发能力。另外可以采用一些技术手段来避免对接口的高并发访问，比如增加缓存，web服务器具备横向拓展的能力， 利用CDN资源，在特殊情况下对推荐服务进行分流、限流、降级等。

### 在线评估第二阶段
第二阶段是用户通过使用推荐算法产生行为(购买、点击、播放等)，主要通过收集分析用户行为日志来评估相关的指标。这一阶段主要是站在平台方角度来思考指标，主要有用户行为相关指标、商业化指标、商家相关指标等。
这里着重介绍用户行为指标，线上评估一般会结合AB测试技术，当采用新算法或者有新的UI交互优化时，将用户分为AB两组，先放一部分流量给测试组(有算法或UI优化的组)，对比组是优化之前的组。如果测试组与对比组在相同指标上有更好的表现, 显著(具备统计显著性)提升了点击或者转化，并且提升是稳定的， 后续逐步将优化拓展到所有用户。

# 四、总结

推荐系统评估要想落地取得较好的效果，真实的反馈推荐系统的问题，为推荐系统提供优化的建议

### 可以关注以下问题：

#### 离线评估准确度高的模型，在线评估不一定高
离线评估会受到可用的数据及评估方法的影响，同时，模型上线会受到各种相关变量的干扰，导致线上评估跟离线评估结果不一致。所以有必要引入AB测试减少新算法上线对用户体验的影响

#### 推荐系统寻求的是一个全局最优化的方案(解)
推荐模型求解是满足整体最优的一个过程(推荐算法如矩阵分解就是将所有用户行为整合进来作为目标函数，再求解误差最小时用户对未知“标的物”的评分)，不能保证每个用户都是预测最准的。

#### 推荐系统是一个多目标优化问题
推荐系统需要平衡很多因素(商业、用户体验、技术实现、资金、人力等)，怎么做好平衡是一种哲学。在公司不同阶段，倾向性也不一样，创业前期可能以用户体验为主，需要大力发展用户，当用户量足够多后，可能会侧重商业变现(推荐更多的付费视频，在搜索列表中插入较多广告等)，尽快让公司开始盈利。

#### AB测试平台对推荐评估的巨大价值值得考量
推荐系统在线评估强烈依赖于AB测试来得出信服的结论，所以一套完善的推荐系统解决方案一定要保证搭建一套高效易用的AB测试框架，让推荐系统的优化有据可循，通过数据驱动来让推荐系统真正做到闭环。

#### 要重视线上用户行为及商业变现方面的评估
线上评估更能真实反映产品的情况，所以在实际推荐系统评估中，要更加重视线上效果评估，它能够很好的将用户的行为跟商业指标结合起来，它的价值一定大于线下评估，需要推荐开发人员及相关产品经理花费更多的时间和精力。

### 可以继续研究的领域

#### 保护用户隐私
在现有经典的推荐算法中，大多数是利用用户数据产生推荐的，如果要得到高准确率、高用户体验的推荐势必会更深层收集与挖掘用户数据，这就给用户隐私带来了一定的威胁。一个好的RS，理应既提供准确、合理的推荐，又确保用户信息不被恶意用户随意获取。现有的文献中，考虑到用户隐私的推荐算法研究较少。因此在对系统进行评估时应考虑是否保护了用户隐私。

#### 推荐算法的时间效率
快节奏的时代“快”是一种趋势。推荐算法能否在短时间内处理爆炸量的信息，给出实时的主动推荐也是值得考虑的因素。

#### 算法的普适性
从复杂的指标群中选择合适的指标去评估系统极具挑战性。不同的推荐算法应用在不同的数据集中效果不同，目前未存在能够应用于所有领域的推荐算法。如果推荐算法在不同的数据集得到的效果相差不大，即说明该算法普适性强。

#### 添加客观性生理指标
评测是一个价值判断的过程，在用户调查研究中用户的作答是主观的，未必是用户真实的想法与感受。因此在评价中应引入一些客观性的指标辅助评估，例如，心率、血压、脑成像等，以提高结论的可信度。

# 五、参考文献
#### 1.Zafarani R., Abbasi M.A., Liu H.Recommendation in Social Media.Evaluating Recommendation - Social Media Mining_An Introduction-CUP (2014)
#### 2.刘攀、陈敏刚 .个性化推荐系统评估.南昌大学学报
#### 3.刘春霞，武玲梅，谢小红.推荐系统评估研究综述.研究与开发
#### 4.高 斐,陈德礼,严 涛.基于内容推荐和协同过滤算法实现个性化评估.安徽大学学报（自然科学版）
#### 5、构建企业级推荐系统——推荐系统评估.数据与智能


